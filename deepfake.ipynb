{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary packages\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tqdm\n",
    "import PIL\n",
    "import sklearn\n",
    "import streamlit\n",
    "\n",
    "print(\"✅ All packages are imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU Information\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and Preprocess Dataset \n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 for ResNet50\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Define dataset paths\n",
    "data_dir = r\"D:\\Projects\\Dataset\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=f\"{data_dir}/Train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=f\"{data_dir}/Validation\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=f\"{data_dir}/Test\", transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"✅ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✅ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"✅ Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Check class names\n",
    "print(f\"Classes: {train_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and Modify ResNet50 Model\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "num_ftrs = resnet50.fc.in_features\n",
    "resnet50.fc = nn.Linear(num_ftrs, 2)  # 2 classes: Real & Fake\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50 = resnet50.to(device)\n",
    "\n",
    "print(\"✅ ResNet50 model is ready for fine-tuning on Deepfake dataset!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation & DataLoader Setup\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to match ResNet50 input\n",
    "        transforms.RandomHorizontalFlip(),  # Augmentation for better learning\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define dataset directory\n",
    "data_dir = \"D:/Projects/Dataset\"\n",
    "\n",
    "# Load datasets\n",
    "datasets_dict = {\n",
    "    \"train\": datasets.ImageFolder(root=f\"{data_dir}/Train\", transform=data_transforms[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(root=f\"{data_dir}/Validation\", transform=data_transforms[\"val\"]),\n",
    "    \"test\": datasets.ImageFolder(root=f\"{data_dir}/Test\", transform=data_transforms[\"test\"])\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust batch size based on available GPU memory\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(datasets_dict[\"train\"], batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True),\n",
    "    \"val\": DataLoader(datasets_dict[\"val\"], batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
    "    \"test\": DataLoader(datasets_dict[\"test\"], batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "}\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"✅ Training samples: {len(datasets_dict['train'])}\")\n",
    "print(f\"✅ Validation samples: {len(datasets_dict['val'])}\")\n",
    "print(f\"✅ Test samples: {len(datasets_dict['test'])}\")\n",
    "print(f\"Classes: {datasets_dict['train'].classes}\")  # Should output ['Fake', 'Real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Pretrained ResNet50 Model\n",
    "def load_resnet50():\n",
    "    model = models.resnet50(weights=\"IMAGENET1K_V1\")  # Load pretrained model\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)  # Modify last layer for binary classification (Fake vs Real)\n",
    "    return model.to(device)  # Move model to GPU if available\n",
    "\n",
    "# Initialize model\n",
    "model = load_resnet50()\n",
    "print(\"✅ Model Loaded and Modified Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔥 Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN [1/5]: 100%|██████████| 4376/4376 [38:41<00:00,  1.89it/s, acc=97.4, loss=0.000104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train Loss: 0.0666, Accuracy: 97.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAL [1/5]: 100%|██████████| 1233/1233 [03:32<00:00,  5.80it/s, acc=97, loss=0.1]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Val Loss: 0.0753, Accuracy: 97.04%\n",
      "🎯 Best model saved!\n",
      "\n",
      "🔥 Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN [2/5]: 100%|██████████| 4376/4376 [37:39<00:00,  1.94it/s, acc=98.4, loss=3.42]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train Loss: 0.0417, Accuracy: 98.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAL [2/5]: 100%|██████████| 1233/1233 [02:47<00:00,  7.35it/s, acc=96.7, loss=0.0658]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Val Loss: 0.0900, Accuracy: 96.66%\n",
      "\n",
      "🔥 Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN [3/5]: 100%|██████████| 4376/4376 [37:18<00:00,  1.95it/s, acc=98.5, loss=0.466]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train Loss: 0.0365, Accuracy: 98.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAL [3/5]: 100%|██████████| 1233/1233 [02:38<00:00,  7.77it/s, acc=98, loss=0.00914]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Val Loss: 0.0595, Accuracy: 97.98%\n",
      "🎯 Best model saved!\n",
      "\n",
      "🔥 Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN [4/5]: 100%|██████████| 4376/4376 [36:34<00:00,  1.99it/s, acc=98.8, loss=1.48]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train Loss: 0.0310, Accuracy: 98.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAL [4/5]: 100%|██████████| 1233/1233 [02:38<00:00,  7.80it/s, acc=97.9, loss=0.0922]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Val Loss: 0.0609, Accuracy: 97.94%\n",
      "\n",
      "🔥 Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAIN [5/5]: 100%|██████████| 4376/4376 [35:50<00:00,  2.04it/s, acc=98.8, loss=5.3e-6]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train Loss: 0.0289, Accuracy: 98.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VAL [5/5]: 100%|██████████| 1233/1233 [02:45<00:00,  7.44it/s, acc=98.1, loss=0.0972] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Val Loss: 0.0585, Accuracy: 98.07%\n",
      "🎯 Best model saved!\n",
      "\n",
      "🚀 Training Completed in 200.47 minutes!\n"
     ]
    }
   ],
   "source": [
    "#Finetuning the Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, dataloaders, criterion, optimizer, epochs=5):\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0.0  # Track best validation accuracy\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n🔥 Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            # Use tqdm for progress tracking\n",
    "            loop = tqdm(dataloaders[phase], desc=f\"{phase.upper()} [{epoch+1}/{epochs}]\", leave=True)\n",
    "\n",
    "            for images, labels in loop:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "                loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = 100 * correct / total\n",
    "\n",
    "            print(f\"✅ {phase.capitalize()} Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "            # Save best model\n",
    "            if phase == 'val' and epoch_acc > best_val_acc:\n",
    "                best_val_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "                print(\"🎯 Best model saved!\")\n",
    "\n",
    "    print(f\"\\n🚀 Training Completed in {(time.time() - start_time) / 60:.2f} minutes!\")\n",
    "\n",
    "# Start Training\n",
    "train_model(model, dataloaders, criterion, optimizer, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahi4\\AppData\\Local\\Temp\\ipykernel_7072\\911118414.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n",
      "🔍 Evaluating on Test Set: 100%|██████████| 341/341 [00:58<00:00,  5.82it/s, acc=87.3, loss=0.0526]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Test Loss: 0.4384, Test Accuracy: 87.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the best trained model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the test function\n",
    "def evaluate_model(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"🔍 Evaluating on Test Set\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loop:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "\n",
    "    test_loss = running_loss / len(dataloader.dataset)\n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"\\n🎯 Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Run evaluation on the test set\n",
    "evaluate_model(model, dataloaders[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahi4\\AppData\\Local\\Temp\\ipykernel_7072\\1689452947.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Projects/Dataset/Test/Fake/example.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Test with an image (change path accordingly)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m test_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Projects/Dataset/Test/Fake/example.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with actual test image\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 19\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_image\u001b[39m(image_path):\n\u001b[1;32m---> 19\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     image \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\mahi4\\miniconda3\\envs\\pytorch_gpu\\lib\\site-packages\\PIL\\Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Projects/Dataset/Test/Fake/example.jpg'"
     ]
    }
   ],
   "source": [
    "#Final code\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for a single image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Function to predict an image\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    class_names = [\"Fake\", \"Real\"]  # Adjust according to your dataset\n",
    "    print(f\"🔍 Prediction: {class_names[predicted.item()]}\")\n",
    "\n",
    "# Test with an image (change path accordingly)\n",
    "test_image_path = \"D:/Projects/Dataset/Test/Fake/example.jpg\"  # Replace with actual test image\n",
    "predict_image(test_image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
